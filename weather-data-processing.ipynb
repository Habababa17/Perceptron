{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install numpy\n",
    "# %pip install scikit-learn\n",
    "# %pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:24.574362Z",
     "iopub.status.busy": "2024-12-29T16:54:24.573960Z",
     "iopub.status.idle": "2024-12-29T16:54:25.867960Z",
     "shell.execute_reply": "2024-12-29T16:54:25.866721Z",
     "shell.execute_reply.started": "2024-12-29T16:54:24.574325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patry\\anaconda3\\envs\\weather\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import kagglehub\n",
    "import cupy as cp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:25.869990Z",
     "iopub.status.busy": "2024-12-29T16:54:25.869419Z",
     "iopub.status.idle": "2024-12-29T16:54:29.158623Z",
     "shell.execute_reply": "2024-12-29T16:54:29.157504Z",
     "shell.execute_reply.started": "2024-12-29T16:54:25.869950Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "historical_hourly_weather_data_path = kagglehub.dataset_download('selfishgene/historical-hourly-weather-data')\n",
    "\n",
    "city = \"Portland\"\n",
    "\n",
    "city_attributes = pd.read_csv(f\"{historical_hourly_weather_data_path}/city_attributes.csv\")\n",
    "humidity = pd.read_csv(f\"{historical_hourly_weather_data_path}/humidity.csv\")\n",
    "pressure = pd.read_csv(f\"{historical_hourly_weather_data_path}/pressure.csv\")\n",
    "temperature = pd.read_csv(f\"{historical_hourly_weather_data_path}/temperature.csv\")\n",
    "weather_description = pd.read_csv(f\"{historical_hourly_weather_data_path}/weather_description.csv\")\n",
    "wind_speed = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_speed.csv\")\n",
    "wind_direction = pd.read_csv(f\"{historical_hourly_weather_data_path}/wind_direction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:29.161394Z",
     "iopub.status.busy": "2024-12-29T16:54:29.160941Z",
     "iopub.status.idle": "2024-12-29T16:54:29.178700Z",
     "shell.execute_reply": "2024-12-29T16:54:29.177526Z",
     "shell.execute_reply.started": "2024-12-29T16:54:29.161353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if city not in city_attributes['City'].values:\n",
    "    raise ValueError(f\"City '{city}' does not exist in the data. Available cities are: {city_attributes['City'].unique()}\")\n",
    "\n",
    "selected_city = city_attributes[city_attributes['City'] == city].index[0]\n",
    "data_frames = [humidity, pressure, temperature, weather_description, wind_speed, wind_direction]\n",
    "\n",
    "for i, df in enumerate(data_frames):\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    data_frames[i] = df.iloc[:, selected_city]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:29.180658Z",
     "iopub.status.busy": "2024-12-29T16:54:29.180324Z",
     "iopub.status.idle": "2024-12-29T16:54:29.674041Z",
     "shell.execute_reply": "2024-12-29T16:54:29.673014Z",
     "shell.execute_reply.started": "2024-12-29T16:54:29.180632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "combined_data = pd.concat(data_frames, axis=1)\n",
    "combined_data.columns = [\n",
    "    'humidity', 'pressure', 'temperature', 'weather_description', \n",
    "    'wind_speed', 'wind_direction'\n",
    "]\n",
    "combined_data.index = pd.to_datetime(combined_data.index)\n",
    "# aggregate daily\n",
    "aggregated_data = combined_data.resample('D').agg({\n",
    "    'temperature': 'mean',\n",
    "    'humidity': 'mean',\n",
    "    'wind_speed': ['max', 'mean'],\n",
    "    'pressure': 'mean',\n",
    "    'weather_description': lambda x: x.mode()[0] if not x.mode().empty else np.nan,\n",
    "    'wind_direction': 'mean'\n",
    "})\n",
    "aggregated_data.columns = ['_'.join(col).strip('_') for col in aggregated_data.columns.values]\n",
    "aggregated_data = aggregated_data.rename(columns={\n",
    "    'temperature_mean': 'mean_temperature',\n",
    "    'wind_speed_max': 'max_wind_speed'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:29.675192Z",
     "iopub.status.busy": "2024-12-29T16:54:29.674948Z",
     "iopub.status.idle": "2024-12-29T16:54:29.680544Z",
     "shell.execute_reply": "2024-12-29T16:54:29.679146Z",
     "shell.execute_reply.started": "2024-12-29T16:54:29.675171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def get_wind_direction_for_max_speed(group):\n",
    "#     max_wind_speed_idx = group['wind_speed'].idxmax()\n",
    "#     return group.loc[max_wind_speed_idx, 'wind_direction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:29.681920Z",
     "iopub.status.busy": "2024-12-29T16:54:29.681630Z",
     "iopub.status.idle": "2024-12-29T16:54:30.109542Z",
     "shell.execute_reply": "2024-12-29T16:54:30.108502Z",
     "shell.execute_reply.started": "2024-12-29T16:54:29.681896Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# aggregated_data.rename(columns={'wind_speed': 'wind_speed_max'}, inplace=True)\n",
    "\n",
    "# aggregated_data['wind_direction'] = combined_data.groupby(combined_data.index.date).apply(get_wind_direction_for_max_speed)\n",
    "\n",
    "# aggregated_data['wind_speed_mean'] = combined_data['wind_speed'].resample('D').mean()\n",
    "\n",
    "# encoder = LabelEncoder()\n",
    "# aggregated_data['weather_description'] = encoder.fit_transform(aggregated_data['weather_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:30.110815Z",
     "iopub.status.busy": "2024-12-29T16:54:30.110545Z",
     "iopub.status.idle": "2024-12-29T16:54:30.116792Z",
     "shell.execute_reply": "2024-12-29T16:54:30.115861Z",
     "shell.execute_reply.started": "2024-12-29T16:54:30.110788Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# weather_mapping = dict(enumerate(encoder.classes_))\n",
    "# print(\"Mapping for weather_description:\", weather_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:30.119864Z",
     "iopub.status.busy": "2024-12-29T16:54:30.119559Z",
     "iopub.status.idle": "2024-12-29T16:54:30.142206Z",
     "shell.execute_reply": "2024-12-29T16:54:30.141099Z",
     "shell.execute_reply.started": "2024-12-29T16:54:30.119840Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# aggregated_data['mean_temperature_next_day'] = aggregated_data['temperature'].shift(-1)\n",
    "# aggregated_data['max_wind_speed_next_day'] = aggregated_data['wind_speed_max'].shift(-1)\n",
    "\n",
    "# aggregated_data = aggregated_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:30.144158Z",
     "iopub.status.busy": "2024-12-29T16:54:30.143864Z",
     "iopub.status.idle": "2024-12-29T16:54:30.153837Z",
     "shell.execute_reply": "2024-12-29T16:54:30.152886Z",
     "shell.execute_reply.started": "2024-12-29T16:54:30.144133Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# X = aggregated_data.drop(columns=['mean_temperature_next_day', 'max_wind_speed_next_day'])\n",
    "# y = aggregated_data[['mean_temperature_next_day', 'max_wind_speed_next_day']]\n",
    "\n",
    "# train_size = int(0.8 * len(aggregated_data))\n",
    "# X_train, X_test = X[:train_size], X[train_size:]\n",
    "# y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:30.155495Z",
     "iopub.status.busy": "2024-12-29T16:54:30.155061Z",
     "iopub.status.idle": "2024-12-29T16:54:30.172300Z",
     "shell.execute_reply": "2024-12-29T16:54:30.171295Z",
     "shell.execute_reply.started": "2024-12-29T16:54:30.155450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(\"Train data:\", X_train.shape, y_train.shape)\n",
    "# print(\"Test data:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:30.173978Z",
     "iopub.status.busy": "2024-12-29T16:54:30.173528Z",
     "iopub.status.idle": "2024-12-29T16:54:30.195625Z",
     "shell.execute_reply": "2024-12-29T16:54:30.194359Z",
     "shell.execute_reply.started": "2024-12-29T16:54:30.173940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:30.197273Z",
     "iopub.status.busy": "2024-12-29T16:54:30.196868Z",
     "iopub.status.idle": "2024-12-29T16:54:30.207111Z",
     "shell.execute_reply": "2024-12-29T16:54:30.205597Z",
     "shell.execute_reply.started": "2024-12-29T16:54:30.197244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:30.208969Z",
     "iopub.status.busy": "2024-12-29T16:54:30.208583Z",
     "iopub.status.idle": "2024-12-29T16:54:30.224534Z",
     "shell.execute_reply": "2024-12-29T16:54:30.223181Z",
     "shell.execute_reply.started": "2024-12-29T16:54:30.208941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def create_time_windows(data, window_size=5):\n",
    "#     X, y = [], []\n",
    "#     for i in range(window_size, len(data)):\n",
    "#         X.append(data.iloc[i-window_size:i][['temperature', 'humidity', 'pressure', 'wind_speed_max', 'wind_speed_mean', 'wind_direction', 'weather_description']].values.T)\n",
    "#         y.append(data.iloc[i][['mean_temperature_next_day', 'max_wind_speed_next_day']].values)\n",
    "#     return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:30.226245Z",
     "iopub.status.busy": "2024-12-29T16:54:30.225856Z",
     "iopub.status.idle": "2024-12-29T16:54:32.214422Z",
     "shell.execute_reply": "2024-12-29T16:54:32.213087Z",
     "shell.execute_reply.started": "2024-12-29T16:54:30.226208Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# X, y = create_time_windows(aggregated_data)\n",
    "\n",
    "# train_size = int(0.8 * len(X))\n",
    "# X_train, X_test = X[:train_size], X[train_size:]\n",
    "# y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T16:54:52.861230Z",
     "iopub.status.busy": "2024-12-29T16:54:52.860846Z",
     "iopub.status.idle": "2024-12-29T16:54:52.868867Z",
     "shell.execute_reply": "2024-12-29T16:54:52.867778Z",
     "shell.execute_reply.started": "2024-12-29T16:54:52.861203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(\"Training data windowed:\", X_train.shape, y_train.shape)\n",
    "# print(\"Test data windowed:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_weather_data(data, window_size=3):\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(data) - 1):\n",
    "        X_window = data.iloc[i-window_size:i][[\n",
    "            'mean_temperature', 'humidity_mean', 'pressure_mean', 'max_wind_speed', 'wind_speed_mean', 'wind_direction_mean'\n",
    "        ]].values\n",
    "        y_target = data.iloc[i + 1][['mean_temperature', 'max_wind_speed']].values\n",
    "\n",
    "        # encode wind speed > 6 as binary\n",
    "        y_target[1] = 1 if y_target[1] >= 6 else 0\n",
    "        X.append(X_window)\n",
    "        y.append(y_target)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # normalize X\n",
    "    X_mean = X.mean(axis=(0, 1), keepdims=True)\n",
    "    X_std = X.std(axis=(0, 1), keepdims=True)\n",
    "    X_normalized = (X - X_mean) / (X_std + 1e-9)\n",
    "\n",
    "    return X_normalized, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess_weather_data(aggregated_data, window_size=3)\n",
    "\n",
    "# Split into train/test\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1506, 3, 6)\n",
      "(1506, 2)\n",
      "(377, 3, 6)\n",
      "(377, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.001):\n",
    "    X_train_cp = cp.array(X_train.reshape(X_train.shape[0], -1), dtype=cp.float32)\n",
    "    y_train_cp = cp.array(y_train, dtype=cp.float32)\n",
    "    X_test_cp = cp.array(X_test.reshape(X_test.shape[0], -1), dtype=cp.float32)\n",
    "    y_test_cp = cp.array(y_test, dtype=cp.float32)\n",
    "\n",
    "    model.train(X_train_cp, y_train_cp, epochs, learning_rate)\n",
    "\n",
    "    predictions = model.predict(X_test_cp)\n",
    "    \n",
    "    mae = cp.mean(cp.abs(predictions[:, 0] - y_test_cp[:, 0]))\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc = roc_auc_score(cp.asnumpy(y_test_cp[:, 1]), cp.asnumpy(predictions[:, 1]))\n",
    "\n",
    "    print(f\"Test Regression MAE: {mae}\")\n",
    "    print(f\"Test Classification AUC: {auc}\")\n",
    "\n",
    "    return mae, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Regression Loss: 285.1441650390625, Classification AUC: 0.45829492829190077, Learning Rate: 0.0001\n",
      "Epoch 100, Regression Loss: 283.267333984375, Classification AUC: 0.5932474405983337, Learning Rate: 0.0001\n",
      "Epoch 200, Regression Loss: 281.0892028808594, Classification AUC: 0.6626007781926644, Learning Rate: 0.0001\n",
      "Epoch 300, Regression Loss: 278.3092346191406, Classification AUC: 0.6860968143438625, Learning Rate: 0.0001\n",
      "Epoch 400, Regression Loss: 274.6795959472656, Classification AUC: 0.699525683721869, Learning Rate: 0.0001\n",
      "Epoch 500, Regression Loss: 270.03643798828125, Classification AUC: 0.7158587591527343, Learning Rate: 0.0001\n",
      "Epoch 600, Regression Loss: 264.24365234375, Classification AUC: 0.7274867965149527, Learning Rate: 0.0001\n",
      "Epoch 700, Regression Loss: 257.1720886230469, Classification AUC: 0.7358383512183089, Learning Rate: 0.0001\n",
      "Epoch 800, Regression Loss: 248.69337463378906, Classification AUC: 0.7422231192742849, Learning Rate: 0.0001\n",
      "Epoch 900, Regression Loss: 238.6808319091797, Classification AUC: 0.7460692299929358, Learning Rate: 0.0001\n",
      "Epoch 1000, Regression Loss: 227.00442504882812, Classification AUC: 0.7504311456476156, Learning Rate: 0.0001\n",
      "Epoch 1100, Regression Loss: 213.53143310546875, Classification AUC: 0.754393873134412, Learning Rate: 0.0001\n",
      "Epoch 1200, Regression Loss: 198.14418029785156, Classification AUC: 0.756768818470302, Learning Rate: 0.0001\n",
      "Epoch 1300, Regression Loss: 180.72145080566406, Classification AUC: 0.7579955371659882, Learning Rate: 0.0001\n",
      "Epoch 1400, Regression Loss: 161.52117919921875, Classification AUC: 0.7585584373353068, Learning Rate: 0.0001\n",
      "Epoch 1500, Regression Loss: 141.55410766601562, Classification AUC: 0.7589351992016238, Learning Rate: 0.0001\n",
      "Epoch 1600, Regression Loss: 121.80567169189453, Classification AUC: 0.7592177706013613, Learning Rate: 0.0001\n",
      "Epoch 1700, Regression Loss: 103.1771240234375, Classification AUC: 0.7588791334477074, Learning Rate: 0.0001\n",
      "Epoch 1800, Regression Loss: 88.95167541503906, Classification AUC: 0.7577802446709501, Learning Rate: 0.0001\n",
      "Epoch 1900, Regression Loss: 80.17630767822266, Classification AUC: 0.7549208912212242, Learning Rate: 0.0001\n",
      "Epoch 2000, Regression Loss: 75.01655578613281, Classification AUC: 0.7524607259393818, Learning Rate: 0.0001\n",
      "Epoch 2100, Regression Loss: 71.505615234375, Classification AUC: 0.7520548098810285, Learning Rate: 0.0001\n",
      "Epoch 2200, Regression Loss: 68.83385467529297, Classification AUC: 0.7530707213419898, Learning Rate: 0.0001\n",
      "Epoch 2300, Regression Loss: 66.37088012695312, Classification AUC: 0.7569527141431471, Learning Rate: 0.0001\n",
      "Epoch 2400, Regression Loss: 63.91050338745117, Classification AUC: 0.7605588634350365, Learning Rate: 0.0001\n",
      "Epoch 2500, Regression Loss: 61.43373489379883, Classification AUC: 0.7616622374721072, Learning Rate: 0.0001\n",
      "Epoch 2600, Regression Loss: 58.91875076293945, Classification AUC: 0.7619380809813749, Learning Rate: 0.0001\n",
      "Epoch 2700, Regression Loss: 56.325557708740234, Classification AUC: 0.7613123871676702, Learning Rate: 0.0001\n",
      "Epoch 2800, Regression Loss: 53.66744613647461, Classification AUC: 0.7620345140781107, Learning Rate: 0.0001\n",
      "Epoch 2900, Regression Loss: 50.95041275024414, Classification AUC: 0.7610298157679326, Learning Rate: 0.0001\n",
      "Epoch 3000, Regression Loss: 48.169559478759766, Classification AUC: 0.7596079882486181, Learning Rate: 0.0001\n",
      "Epoch 3100, Regression Loss: 45.34299850463867, Classification AUC: 0.7580493602897478, Learning Rate: 0.0001\n",
      "Epoch 3200, Regression Loss: 42.48249816894531, Classification AUC: 0.7557349659680874, Learning Rate: 0.0001\n",
      "Epoch 3300, Regression Loss: 39.68705749511719, Classification AUC: 0.7536582904430316, Learning Rate: 0.0001\n",
      "Epoch 3400, Regression Loss: 37.03684997558594, Classification AUC: 0.7519852883461724, Learning Rate: 0.0001\n",
      "Epoch 3500, Regression Loss: 34.58790969848633, Classification AUC: 0.749937767013153, Learning Rate: 0.0001\n",
      "Epoch 3600, Regression Loss: 32.26845932006836, Classification AUC: 0.7463002208990703, Learning Rate: 0.0001\n",
      "Epoch 3700, Regression Loss: 29.94533920288086, Classification AUC: 0.7444904183626557, Learning Rate: 0.0001\n",
      "Epoch 3800, Regression Loss: 27.570802688598633, Classification AUC: 0.7407923212343436, Learning Rate: 0.0001\n",
      "Epoch 3900, Regression Loss: 25.19563102722168, Classification AUC: 0.7377064621387964, Learning Rate: 0.0001\n",
      "Epoch 4000, Regression Loss: 22.86260414123535, Classification AUC: 0.7320034536504413, Learning Rate: 0.0001\n",
      "Epoch 4100, Regression Loss: 20.54784393310547, Classification AUC: 0.7281102476985007, Learning Rate: 0.0001\n",
      "Epoch 4200, Regression Loss: 18.25004005432129, Classification AUC: 0.7252912615915946, Learning Rate: 0.0001\n",
      "Epoch 4300, Regression Loss: 16.0203914642334, Classification AUC: 0.7218196701091041, Learning Rate: 0.0001\n",
      "Epoch 4400, Regression Loss: 13.889446258544922, Classification AUC: 0.7201735795741246, Learning Rate: 0.0001\n",
      "Epoch 4500, Regression Loss: 11.827685356140137, Classification AUC: 0.7212051894461824, Learning Rate: 0.0001\n",
      "Epoch 4600, Regression Loss: 9.907430648803711, Classification AUC: 0.7231271234904295, Learning Rate: 0.0001\n",
      "Epoch 4700, Regression Loss: 8.214680671691895, Classification AUC: 0.7261927989145669, Learning Rate: 0.0001\n",
      "Epoch 4800, Regression Loss: 6.773586750030518, Classification AUC: 0.7295971114923582, Learning Rate: 0.0001\n",
      "Epoch 4900, Regression Loss: 5.622929573059082, Classification AUC: 0.7296576625065877, Learning Rate: 0.0001\n",
      "Epoch 5000, Regression Loss: 4.823184967041016, Classification AUC: 0.7305883540215966, Learning Rate: 1e-05\n",
      "Epoch 5100, Regression Loss: 4.714077949523926, Classification AUC: 0.7319395386909767, Learning Rate: 1e-05\n",
      "Epoch 5200, Regression Loss: 4.607599258422852, Classification AUC: 0.7334656485125755, Learning Rate: 1e-05\n",
      "Epoch 5300, Regression Loss: 4.5054850578308105, Classification AUC: 0.7344995010147901, Learning Rate: 1e-05\n",
      "Epoch 5400, Regression Loss: 4.405186653137207, Classification AUC: 0.7356051176820174, Learning Rate: 1e-05\n",
      "Epoch 5500, Regression Loss: 4.308729648590088, Classification AUC: 0.7368632331998969, Learning Rate: 1e-05\n",
      "Epoch 5600, Regression Loss: 4.214537620544434, Classification AUC: 0.7378298067974122, Learning Rate: 1e-05\n",
      "Epoch 5700, Regression Loss: 4.122395992279053, Classification AUC: 0.7388255345869635, Learning Rate: 1e-05\n",
      "Epoch 5800, Regression Loss: 4.0323591232299805, Classification AUC: 0.7398706002399613, Learning Rate: 1e-05\n",
      "Epoch 5900, Regression Loss: 3.9448163509368896, Classification AUC: 0.7406734618360413, Learning Rate: 1e-05\n",
      "Epoch 6000, Regression Loss: 3.8611598014831543, Classification AUC: 0.7417117995985693, Learning Rate: 1e-05\n",
      "Epoch 6100, Regression Loss: 3.781491756439209, Classification AUC: 0.7425415727565289, Learning Rate: 1e-05\n",
      "Epoch 6200, Regression Loss: 3.7043211460113525, Classification AUC: 0.7434363821890313, Learning Rate: 1e-05\n",
      "Epoch 6300, Regression Loss: 3.6296393871307373, Classification AUC: 0.7440934728249291, Learning Rate: 1e-05\n",
      "Epoch 6400, Regression Loss: 3.557286500930786, Classification AUC: 0.7449479149146119, Learning Rate: 1e-05\n",
      "Epoch 6500, Regression Loss: 3.4864883422851562, Classification AUC: 0.7456341597425461, Learning Rate: 1e-05\n",
      "Epoch 6600, Regression Loss: 3.419229745864868, Classification AUC: 0.7463316177212634, Learning Rate: 1e-05\n",
      "Epoch 6700, Regression Loss: 3.3541040420532227, Classification AUC: 0.747091869344367, Learning Rate: 1e-05\n",
      "Epoch 6800, Regression Loss: 3.2904410362243652, Classification AUC: 0.7477758715421446, Learning Rate: 1e-05\n",
      "Epoch 6900, Regression Loss: 3.227947950363159, Classification AUC: 0.7485899462890078, Learning Rate: 1e-05\n",
      "Epoch 7000, Regression Loss: 3.1664373874664307, Classification AUC: 0.7493614110628946, Learning Rate: 1e-05\n",
      "Epoch 7100, Regression Loss: 3.1073193550109863, Classification AUC: 0.7501597873986612, Learning Rate: 1e-05\n",
      "Epoch 7200, Regression Loss: 3.049421787261963, Classification AUC: 0.7508976127201983, Learning Rate: 1e-05\n",
      "Epoch 7300, Regression Loss: 2.9950106143951416, Classification AUC: 0.7517027169464349, Learning Rate: 1e-05\n",
      "Epoch 7400, Regression Loss: 2.9431581497192383, Classification AUC: 0.7526423789820702, Learning Rate: 1e-05\n",
      "Epoch 7500, Regression Loss: 2.893176555633545, Classification AUC: 0.75343851268768, Learning Rate: 1e-05\n",
      "Epoch 7600, Regression Loss: 2.846689462661743, Classification AUC: 0.75424810217423, Learning Rate: 1e-05\n",
      "Epoch 7700, Regression Loss: 2.80196213722229, Classification AUC: 0.754927619111694, Learning Rate: 1e-05\n",
      "Epoch 7800, Regression Loss: 2.759960412979126, Classification AUC: 0.7558201859140401, Learning Rate: 1e-05\n",
      "Epoch 7900, Regression Loss: 2.720449686050415, Classification AUC: 0.7565467980847939, Learning Rate: 1e-05\n",
      "Epoch 8000, Regression Loss: 2.6819705963134766, Classification AUC: 0.757188190309595, Learning Rate: 1e-05\n",
      "Epoch 8100, Regression Loss: 2.6457679271698, Classification AUC: 0.758060573440531, Learning Rate: 1e-05\n",
      "Epoch 8200, Regression Loss: 2.6111459732055664, Classification AUC: 0.7588477366255143, Learning Rate: 1e-05\n",
      "Epoch 8300, Regression Loss: 2.578590154647827, Classification AUC: 0.7597694576198967, Learning Rate: 1e-05\n",
      "Epoch 8400, Regression Loss: 2.5474047660827637, Classification AUC: 0.7605723192159765, Learning Rate: 1e-05\n",
      "Epoch 8500, Regression Loss: 2.518136501312256, Classification AUC: 0.761103822563102, Learning Rate: 1e-05\n",
      "Epoch 8600, Regression Loss: 2.4915008544921875, Classification AUC: 0.7616465390610108, Learning Rate: 1e-05\n",
      "Epoch 8700, Regression Loss: 2.466745138168335, Classification AUC: 0.7623013870667519, Learning Rate: 1e-05\n",
      "Epoch 8800, Regression Loss: 2.4429709911346436, Classification AUC: 0.7628911987979503, Learning Rate: 1e-05\n",
      "Epoch 8900, Regression Loss: 2.419999122619629, Classification AUC: 0.7634148529395275, Learning Rate: 1e-05\n",
      "Epoch 9000, Regression Loss: 2.3982999324798584, Classification AUC: 0.7639138381493816, Learning Rate: 1e-05\n",
      "Epoch 9100, Regression Loss: 2.377096652984619, Classification AUC: 0.7643466657696144, Learning Rate: 1e-05\n",
      "Epoch 9200, Regression Loss: 2.3568785190582275, Classification AUC: 0.7647548244581245, Learning Rate: 1e-05\n",
      "Epoch 9300, Regression Loss: 2.3378047943115234, Classification AUC: 0.765037395857862, Learning Rate: 1e-05\n",
      "Epoch 9400, Regression Loss: 2.3191351890563965, Classification AUC: 0.7653154819972864, Learning Rate: 1e-05\n",
      "Epoch 9500, Regression Loss: 2.301304340362549, Classification AUC: 0.7655330171224812, Learning Rate: 1e-05\n",
      "Epoch 9600, Regression Loss: 2.284496307373047, Classification AUC: 0.7658066180015922, Learning Rate: 1e-05\n",
      "Epoch 9700, Regression Loss: 2.268622636795044, Classification AUC: 0.7660959172917997, Learning Rate: 1e-05\n",
      "Epoch 9800, Regression Loss: 2.2533411979675293, Classification AUC: 0.7663829739518507, Learning Rate: 1e-05\n",
      "Epoch 9900, Regression Loss: 2.238933563232422, Classification AUC: 0.7666431190500219, Learning Rate: 1e-05\n",
      "Epoch 10000, Regression Loss: 2.224623680114746, Classification AUC: 0.7669324183402294, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 10100, Regression Loss: 2.2223060131073, Classification AUC: 0.7669772709433624, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 10200, Regression Loss: 2.2197039127349854, Classification AUC: 0.7670714614099416, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 10300, Regression Loss: 2.217153787612915, Classification AUC: 0.7671656518765207, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 10400, Regression Loss: 2.214611530303955, Classification AUC: 0.76724638656216, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 10500, Regression Loss: 2.2120540142059326, Classification AUC: 0.7672665702335699, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 10600, Regression Loss: 2.209530830383301, Classification AUC: 0.7672957244256062, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 10700, Regression Loss: 2.2069971561431885, Classification AUC: 0.7673293638779561, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 10800, Regression Loss: 2.2045047283172607, Classification AUC: 0.7673944001524988, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 10900, Regression Loss: 2.2019901275634766, Classification AUC: 0.7675110169206445, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 11000, Regression Loss: 2.1994853019714355, Classification AUC: 0.7675760531951874, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 11100, Regression Loss: 2.197014570236206, Classification AUC: 0.7676119352776938, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 11200, Regression Loss: 2.1945853233337402, Classification AUC: 0.7676971552236462, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 11300, Regression Loss: 2.1921567916870117, Classification AUC: 0.7677307946759959, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 11400, Regression Loss: 2.18977952003479, Classification AUC: 0.7677734046489723, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 11500, Regression Loss: 2.1873860359191895, Classification AUC: 0.7678025588410088, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 11600, Regression Loss: 2.185014247894287, Classification AUC: 0.7678361982933585, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 11700, Regression Loss: 2.182664632797241, Classification AUC: 0.7678967493075879, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 11800, Regression Loss: 2.1803219318389893, Classification AUC: 0.7679236608694677, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 11900, Regression Loss: 2.1779890060424805, Classification AUC: 0.7680178513360468, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 12000, Regression Loss: 2.175687313079834, Classification AUC: 0.7680828876105897, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 12100, Regression Loss: 2.1734092235565186, Classification AUC: 0.7680985860216861, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 12200, Regression Loss: 2.1711156368255615, Classification AUC: 0.7681501665152891, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 12300, Regression Loss: 2.168856143951416, Classification AUC: 0.7682376290913983, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 12400, Regression Loss: 2.166632652282715, Classification AUC: 0.7683609737500139, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 12500, Regression Loss: 2.1644113063812256, Classification AUC: 0.7684237673944001, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 12600, Regression Loss: 2.1621804237365723, Classification AUC: 0.7685179578609792, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 12700, Regression Loss: 2.1599690914154053, Classification AUC: 0.7685874793958353, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 12800, Regression Loss: 2.1577258110046387, Classification AUC: 0.7686457877799082, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 12900, Regression Loss: 2.155475616455078, Classification AUC: 0.7687366143012525, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 13000, Regression Loss: 2.1532599925994873, Classification AUC: 0.7687982866305603, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 13100, Regression Loss: 2.151041269302368, Classification AUC: 0.7688722934257297, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 13200, Regression Loss: 2.1488358974456787, Classification AUC: 0.7689193886590193, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 13300, Regression Loss: 2.1466197967529297, Classification AUC: 0.7690135791255985, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 13400, Regression Loss: 2.1444129943847656, Classification AUC: 0.769060674358888, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 13500, Regression Loss: 2.1422181129455566, Classification AUC: 0.7691324385239009, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 13600, Regression Loss: 2.140004873275757, Classification AUC: 0.7691728058667207, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 13700, Regression Loss: 2.137815475463867, Classification AUC: 0.7692512979222031, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 13800, Regression Loss: 2.1356725692749023, Classification AUC: 0.7693073636761194, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 13900, Regression Loss: 2.133556842803955, Classification AUC: 0.7693836131014453, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 14000, Regression Loss: 2.131455421447754, Classification AUC: 0.7694508920061448, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 14100, Regression Loss: 2.129373073577881, Classification AUC: 0.7695293840616275, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 14200, Regression Loss: 2.1273436546325684, Classification AUC: 0.769565266144134, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 14300, Regression Loss: 2.125338554382324, Classification AUC: 0.7696527287202432, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 14400, Regression Loss: 2.1233551502227783, Classification AUC: 0.7697267355154125, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 14500, Regression Loss: 2.1213765144348145, Classification AUC: 0.7698141980915219, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 14600, Regression Loss: 2.1194190979003906, Classification AUC: 0.7698657785851247, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 14700, Regression Loss: 2.11747407913208, Classification AUC: 0.7699285722295106, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 14800, Regression Loss: 2.1155178546905518, Classification AUC: 0.7700093069151501, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 14900, Regression Loss: 2.113556146621704, Classification AUC: 0.7700877989706327, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 15000, Regression Loss: 2.1116092205047607, Classification AUC: 0.7701662910261153, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 15100, Regression Loss: 2.1114089488983154, Classification AUC: 0.7701595631356455, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 15200, Regression Loss: 2.1112148761749268, Classification AUC: 0.7701595631356455, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 15300, Regression Loss: 2.111017942428589, Classification AUC: 0.7701685336562719, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 15400, Regression Loss: 2.1108224391937256, Classification AUC: 0.7701842320673686, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 15500, Regression Loss: 2.110626459121704, Classification AUC: 0.7701864746975253, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 15600, Regression Loss: 2.1104297637939453, Classification AUC: 0.7702010517935434, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 15700, Regression Loss: 2.1102349758148193, Classification AUC: 0.7701887173276819, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 15800, Regression Loss: 2.1100404262542725, Classification AUC: 0.7702021731086219, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 15900, Regression Loss: 2.10984468460083, Classification AUC: 0.770213386259405, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 16000, Regression Loss: 2.1096489429473877, Classification AUC: 0.770220114149875, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 16100, Regression Loss: 2.109452724456787, Classification AUC: 0.7702245994101883, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 16200, Regression Loss: 2.1092562675476074, Classification AUC: 0.7702156288895617, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 16300, Regression Loss: 2.109060764312744, Classification AUC: 0.770220114149875, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 16400, Regression Loss: 2.1088647842407227, Classification AUC: 0.7702380551911281, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 16500, Regression Loss: 2.108670234680176, Classification AUC: 0.7702492683419113, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 16600, Regression Loss: 2.1084725856781006, Classification AUC: 0.7702582388625379, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 16700, Regression Loss: 2.1082780361175537, Classification AUC: 0.7702559962323813, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 16800, Regression Loss: 2.1080808639526367, Classification AUC: 0.7702582388625377, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 16900, Regression Loss: 2.1078855991363525, Classification AUC: 0.7702739372736345, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 17000, Regression Loss: 2.107689142227173, Classification AUC: 0.7702806651641043, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 17100, Regression Loss: 2.107492208480835, Classification AUC: 0.7702873930545744, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 17200, Regression Loss: 2.107295274734497, Classification AUC: 0.770296363575201, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 17300, Regression Loss: 2.1071009635925293, Classification AUC: 0.7703008488355142, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 17400, Regression Loss: 2.1069037914276123, Classification AUC: 0.7703143046164542, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 17500, Regression Loss: 2.1067075729370117, Classification AUC: 0.770327760397394, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 17600, Regression Loss: 2.106511116027832, Classification AUC: 0.7703457014386473, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 17700, Regression Loss: 2.106316328048706, Classification AUC: 0.7703569145894305, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 17800, Regression Loss: 2.1061201095581055, Classification AUC: 0.7703569145894303, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 17900, Regression Loss: 2.105926275253296, Classification AUC: 0.7703770982608402, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 18000, Regression Loss: 2.1057300567626953, Classification AUC: 0.7703838261513102, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 18100, Regression Loss: 2.1055381298065186, Classification AUC: 0.7704017671925635, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 18200, Regression Loss: 2.1053404808044434, Classification AUC: 0.7704017671925634, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 18300, Regression Loss: 2.105149030685425, Classification AUC: 0.7704084950830334, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 18400, Regression Loss: 2.104954719543457, Classification AUC: 0.7704219508639732, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 18500, Regression Loss: 2.104762315750122, Classification AUC: 0.7704197082338167, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 18600, Regression Loss: 2.1045682430267334, Classification AUC: 0.7704443771655397, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 18700, Regression Loss: 2.1043782234191895, Classification AUC: 0.770448862425853, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 18800, Regression Loss: 2.1041886806488037, Classification AUC: 0.7704466197956964, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 18900, Regression Loss: 2.103998899459839, Classification AUC: 0.7704645608369496, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 19000, Regression Loss: 2.103806972503662, Classification AUC: 0.7704712887274195, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 19100, Regression Loss: 2.103616714477539, Classification AUC: 0.770469046097263, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 19200, Regression Loss: 2.1034255027770996, Classification AUC: 0.7704780166178894, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 19300, Regression Loss: 2.103233814239502, Classification AUC: 0.7704735313575762, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 19400, Regression Loss: 2.1030433177948, Classification AUC: 0.7704757739877329, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 19500, Regression Loss: 2.1028544902801514, Classification AUC: 0.7704847445083594, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 19600, Regression Loss: 2.1026623249053955, Classification AUC: 0.7704757739877328, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 19700, Regression Loss: 2.1024727821350098, Classification AUC: 0.7704937150289859, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 19800, Regression Loss: 2.1022796630859375, Classification AUC: 0.7705116560702393, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 19900, Regression Loss: 2.102091073989868, Classification AUC: 0.7705273544813357, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 20000, Regression Loss: 2.1019020080566406, Classification AUC: 0.7705340823718057, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 20100, Regression Loss: 2.101886510848999, Classification AUC: 0.7705340823718057, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 20200, Regression Loss: 2.101869583129883, Classification AUC: 0.7705340823718057, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 20300, Regression Loss: 2.1018524169921875, Classification AUC: 0.7705329610567274, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 20400, Regression Loss: 2.1018359661102295, Classification AUC: 0.7705318397416491, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 20500, Regression Loss: 2.1018176078796387, Classification AUC: 0.770531839741649, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 20600, Regression Loss: 2.1018009185791016, Classification AUC: 0.7705295971114923, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 20700, Regression Loss: 2.101783275604248, Classification AUC: 0.7705295971114923, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 20800, Regression Loss: 2.1017673015594482, Classification AUC: 0.7705295971114923, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 20900, Regression Loss: 2.1017491817474365, Classification AUC: 0.770531839741649, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 21000, Regression Loss: 2.1017329692840576, Classification AUC: 0.7705340823718057, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 21100, Regression Loss: 2.1017162799835205, Classification AUC: 0.770538567632119, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 21200, Regression Loss: 2.1016979217529297, Classification AUC: 0.770538567632119, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 21300, Regression Loss: 2.1016805171966553, Classification AUC: 0.7705363250019625, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 21400, Regression Loss: 2.1016640663146973, Classification AUC: 0.7705396889471974, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 21500, Regression Loss: 2.1016464233398438, Classification AUC: 0.7705452955225889, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 21600, Regression Loss: 2.1016297340393066, Classification AUC: 0.7705452955225889, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 21700, Regression Loss: 2.1016132831573486, Classification AUC: 0.770549780782902, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 21800, Regression Loss: 2.101595401763916, Classification AUC: 0.7705475381527455, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 21900, Regression Loss: 2.1015775203704834, Classification AUC: 0.7705497807829021, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 22000, Regression Loss: 2.1015610694885254, Classification AUC: 0.7705497807829021, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 22100, Regression Loss: 2.101543664932251, Classification AUC: 0.7705452955225889, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 22200, Regression Loss: 2.1015267372131348, Classification AUC: 0.7705452955225889, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 22300, Regression Loss: 2.1015098094940186, Classification AUC: 0.7705452955225889, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 22400, Regression Loss: 2.101492404937744, Classification AUC: 0.7705452955225889, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 22500, Regression Loss: 2.101475477218628, Classification AUC: 0.7705475381527456, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 22600, Regression Loss: 2.101458787918091, Classification AUC: 0.7705475381527456, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 22700, Regression Loss: 2.101440906524658, Classification AUC: 0.770549780782902, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 22800, Regression Loss: 2.101423501968384, Classification AUC: 0.7705475381527456, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 22900, Regression Loss: 2.1014068126678467, Classification AUC: 0.7705452955225889, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 23000, Regression Loss: 2.1013903617858887, Classification AUC: 0.7705475381527456, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 23100, Regression Loss: 2.101372718811035, Classification AUC: 0.7705452955225889, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 23200, Regression Loss: 2.1013553142547607, Classification AUC: 0.7705452955225889, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 23300, Regression Loss: 2.10133695602417, Classification AUC: 0.7705452955225889, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 23400, Regression Loss: 2.101320266723633, Classification AUC: 0.7705475381527455, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 23500, Regression Loss: 2.1013023853302, Classification AUC: 0.7705497807829021, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 23600, Regression Loss: 2.101285219192505, Classification AUC: 0.7705497807829021, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 23700, Regression Loss: 2.101269006729126, Classification AUC: 0.7705520234130588, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 23800, Regression Loss: 2.101250648498535, Classification AUC: 0.7705565086733721, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 23900, Regression Loss: 2.101234197616577, Classification AUC: 0.7705565086733721, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 24000, Regression Loss: 2.1012167930603027, Classification AUC: 0.7705542660432155, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 24100, Regression Loss: 2.101198673248291, Classification AUC: 0.7705520234130588, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 24200, Regression Loss: 2.101182222366333, Classification AUC: 0.7705542660432153, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 24300, Regression Loss: 2.1011641025543213, Classification AUC: 0.7705542660432153, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 24400, Regression Loss: 2.101148843765259, Classification AUC: 0.7705542660432153, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 24500, Regression Loss: 2.101130723953247, Classification AUC: 0.7705542660432153, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 24600, Regression Loss: 2.1011123657226562, Classification AUC: 0.7705520234130587, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 24700, Regression Loss: 2.101095676422119, Classification AUC: 0.7705520234130587, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 24800, Regression Loss: 2.1010794639587402, Classification AUC: 0.7705520234130587, Learning Rate: 1.0000000000000002e-08\n",
      "Epoch 24900, Regression Loss: 2.1010615825653076, Classification AUC: 0.770549780782902, Learning Rate: 1.0000000000000002e-08\n",
      "Test Regression MAE: 2.9840216636657715\n",
      "Test Classification AUC: 0.5751266670549182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(2.9840217, dtype=float32), np.float64(0.5751266670549182))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from weather_prediction import WeatherPredictionNetwork\n",
    "\n",
    "layers = [X_train.shape[1] * X_train.shape[2], 128, 64, 2]\n",
    "activations = [\"relu\", \"relu\"]\n",
    "model = WeatherPredictionNetwork(layers, activations, seed=42)\n",
    "\n",
    "train_and_evaluate(model, X_train, y_train, X_test, y_test, epochs=25000, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_data(X, y):\n",
    "#     X = X.reshape(X.shape[0], -1)  # Flattening the time-series data\n",
    "#     X = cp.array(X, dtype=cp.float32)\n",
    "#     y = cp.array(y, dtype=cp.float32)\n",
    "#     return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from network_test import Network\n",
    "# # Dataset preparation\n",
    "# X_train = np.random.rand(1504, 7, 5)  # Training data (1504 samples, 7 days, 5 features per day)\n",
    "# y_train = np.random.rand(1504, 2)     # Training targets (1504 samples, 2 output features)\n",
    "# X_test = np.random.rand(377, 7, 5)    # Test data\n",
    "# y_test = np.random.rand(377, 2)       # Test targets\n",
    "\n",
    "# # Preprocess the data\n",
    "# X_train, y_train = preprocess_data(X_train, y_train)\n",
    "# X_test, y_test = preprocess_data(X_test, y_test)\n",
    "\n",
    "# # Initialize and train the network\n",
    "# activations = [\"relu\", \"relu\", \"relu\"] \n",
    "# nn = Network([X_train.shape[1], 64, 32, y_train.shape[1]], activations, loss_function=\"mse\", seed=123)\n",
    "# nn.train(X_train, y_train, epochs=500, learning_rate=0.001)\n",
    "\n",
    "# # Predict and evaluate\n",
    "# predictions = nn.predict(X_test)\n",
    "# print(\"Predictions (first 5):\", predictions[:5].get())\n",
    "# print(\"Actual (first 5):\", y_test[:5].get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cupy as cp\n",
    "# from network_test import Network\n",
    "\n",
    "# # Dataset preparation\n",
    "# X_train = np.random.rand(1504, 7, 5)  # Training data (1504 samples, 7 days, 5 features per day)\n",
    "# y_train = np.random.rand(1504, 2)     # Training targets (1504 samples, 2 output features)\n",
    "# X_test = np.random.rand(377, 7, 5)    # Test data\n",
    "# y_test = np.random.rand(377, 2)       # Test targets\n",
    "\n",
    "# # Preprocess the data (no flattening for time-series)\n",
    "# def preprocess_data(X, y):\n",
    "#     # Flattening the time-series data to (samples, days*features)\n",
    "#     X = X.reshape(X.shape[0], -1)  # Now shape is (samples, days*features)\n",
    "#     X = cp.array(X, dtype=cp.float32)\n",
    "#     y = cp.array(y, dtype=cp.float32)\n",
    "#     return X, y\n",
    "\n",
    "# # Preprocess training and testing data\n",
    "# X_train, y_train = preprocess_data(X_train, y_train)\n",
    "# X_test, y_test = preprocess_data(X_test, y_test)\n",
    "\n",
    "# # Initialize and train the network\n",
    "# activations = [\"relu\", \"relu\", \"relu\"]  # Relu for hidden layers\n",
    "# nn = Network([X_train.shape[1], 64, 32, y_train.shape[1]], activations, loss_function=\"mse\", seed=123)\n",
    "\n",
    "# # Train the network\n",
    "# nn.train(X_train, y_train, epochs=500, learning_rate=0.001)\n",
    "\n",
    "# # Predict and evaluate\n",
    "# predictions = nn.predict(X_test)\n",
    "# print(\"Predictions (first 5):\", predictions[:5].get())\n",
    "# print(\"Actual (first 5):\", y_test[:5].get())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import cupy as cp\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# class NeuralNetwork:\n",
    "#     def __init__(self, layers, activation=\"relu\", loss=\"mse\"):\n",
    "#         self.layers = layers\n",
    "#         self.activation = activation\n",
    "#         self.loss = loss\n",
    "#         self.weights = []\n",
    "#         self.biases = []\n",
    "#         self.initialize_weights()\n",
    "\n",
    "#     def initialize_weights(self):\n",
    "#         for i in range(len(self.layers) - 1):\n",
    "#             weight = cp.random.randn(self.layers[i], self.layers[i + 1]) * cp.sqrt(2. / self.layers[i])\n",
    "#             bias = cp.zeros((1, self.layers[i + 1]))\n",
    "#             self.weights.append(weight)\n",
    "#             self.biases.append(bias)\n",
    "\n",
    "#     def activate(self, z, activation):\n",
    "#         if activation == \"sigmoid\":\n",
    "#             return 1 / (1 + cp.exp(-z))\n",
    "#         elif activation == \"relu\":\n",
    "#             return cp.maximum(0, z)\n",
    "#         elif activation == \"linear\":\n",
    "#             return z\n",
    "#         else:\n",
    "#             raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "#     def activate_derivative(self, a, activation):\n",
    "#         if activation == \"sigmoid\":\n",
    "#             return a * (1 - a)\n",
    "#         elif activation == \"relu\":\n",
    "#             return (a > 0).astype(cp.float32)\n",
    "#         elif activation == \"linear\":\n",
    "#             return cp.ones_like(a)\n",
    "#         else:\n",
    "#             raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         activations = [X]\n",
    "#         z_values = []\n",
    "\n",
    "#         for i in range(len(self.weights)):\n",
    "#             z = cp.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "#             a = self.activate(z, \"linear\" if i == len(self.weights) - 1 else self.activation)\n",
    "#             z_values.append(z)\n",
    "#             activations.append(a)\n",
    "\n",
    "#         return activations, z_values\n",
    "\n",
    "#     def backward(self, X, y, activations, z_values):\n",
    "#         deltas = []\n",
    "#         delta = activations[-1] - y\n",
    "#         deltas.append(delta)\n",
    "\n",
    "#         for i in reversed(range(len(self.weights) - 1)):\n",
    "#             delta = cp.dot(deltas[0], self.weights[i + 1].T) * self.activate_derivative(activations[i + 1], self.activation)\n",
    "#             deltas.insert(0, delta)\n",
    "\n",
    "#         weight_grads = []\n",
    "#         bias_grads = []\n",
    "\n",
    "#         for i in range(len(self.weights)):\n",
    "#             weight_grad = cp.dot(activations[i].T, deltas[i]) / X.shape[0]\n",
    "#             bias_grad = cp.mean(deltas[i], axis=0, keepdims=True)\n",
    "#             weight_grads.append(weight_grad)\n",
    "#             bias_grads.append(bias_grad)\n",
    "\n",
    "#         return weight_grads, bias_grads\n",
    "\n",
    "#     def update_parameters(self, weight_grads, bias_grads, learning_rate):\n",
    "#         for i in range(len(self.weights)):\n",
    "#             self.weights[i] -= learning_rate * weight_grads[i]\n",
    "#             self.biases[i] -= learning_rate * bias_grads[i]\n",
    "\n",
    "#     def fit(self, X, y, epochs=1000, learning_rate=0.01):\n",
    "#         for epoch in range(epochs):\n",
    "#             activations, z_values = self.forward(X)\n",
    "#             weight_grads, bias_grads = self.backward(X, y, activations, z_values)\n",
    "#             self.update_parameters(weight_grads, bias_grads, learning_rate)\n",
    "\n",
    "#             if epoch % 100 == 0:\n",
    "#                 loss = self.calculate_loss(y, activations[-1])\n",
    "#                 print(f\"Epoch {epoch}/{epochs} - Loss: {loss}\")\n",
    "\n",
    "#     def calculate_loss(self, y, y_pred):\n",
    "#         if self.loss == \"mse\":\n",
    "#             return cp.mean((y - y_pred) ** 2)\n",
    "#         else:\n",
    "#             raise ValueError(\"Unsupported loss function\")\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         activations, _ = self.forward(X)\n",
    "#         return activations[-1]\n",
    "\n",
    "# def preprocess_data(X, y):\n",
    "#     X = X.reshape(X.shape[0], -1)  # Flatten time-series data\n",
    "#     mean_X = np.mean(X, axis=0)\n",
    "#     std_X = np.std(X, axis=0)\n",
    "#     X_normalized = (X - mean_X) / std_X  # Normalize inputs\n",
    "\n",
    "#     mean_y = np.mean(y, axis=0)\n",
    "#     std_y = np.std(y, axis=0)\n",
    "#     y_normalized = (y - mean_y) / std_y  # Normalize targets\n",
    "\n",
    "#     return (\n",
    "#         cp.array(X_normalized, dtype=cp.float32),\n",
    "#         cp.array(y_normalized, dtype=cp.float32),\n",
    "#         mean_X, std_X,\n",
    "#         mean_y, std_y\n",
    "#     )\n",
    "\n",
    "# def denormalize(predictions, mean_y, std_y):\n",
    "#     mean_y = cp.asnumpy(mean_y)  # Convert cupy to numpy\n",
    "#     std_y = cp.asnumpy(std_y)    # Convert cupy to numpy\n",
    "#     predictions = np.array(predictions)  # Ensure numpy array\n",
    "#     return predictions * std_y + mean_y\n",
    "\n",
    "\n",
    "\n",
    "# # Dataset preparation\n",
    "# # X_train = np.random.rand(1504, 7, 5)\n",
    "# # y_train = np.random.rand(1504, 2)\n",
    "# # X_test = np.random.rand(377, 7, 5)\n",
    "# # y_test = np.random.rand(377, 2)\n",
    "\n",
    "# # Preprocess the data\n",
    "# X_train, y_train, mean_X, std_X, mean_y, std_y = preprocess_data(X_train, y_train)\n",
    "# X_test, y_test, _, _, _, _ = preprocess_data(X_test, y_test)\n",
    "\n",
    "# # Initialize the neural network\n",
    "# nn = NeuralNetwork(\n",
    "#     layers=[X_train.shape[1], 64, 32, y_train.shape[1]],  # Ensure 2 outputs for temperature and wind speed\n",
    "#     activation=\"relu\",\n",
    "#     loss=\"mse\",\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# nn.fit(X_train, y_train, epochs=15000, learning_rate=0.01)\n",
    "\n",
    "# # Predict normalized values\n",
    "# predictions_normalized = nn.predict(X_test).get()\n",
    "\n",
    "# # Denormalize the predictions\n",
    "# predictions_denormalized = denormalize(predictions_normalized, mean_y, std_y)\n",
    "\n",
    "# # Print results\n",
    "# print(\"Predictions (first 5):\", predictions_denormalized[:5])\n",
    "# print(\"Actual (first 5):\", y_test[:5].get())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7752,
     "sourceId": 11375,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "weather",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
